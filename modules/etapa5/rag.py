"""
ETAPA 5 ‚Äî RAG (Retrieval Augmented Generation)
M√≥dulo para ensinar sistemas RAG e bancos vetoriais
"""
import streamlit as st


def render_etapa5():
    """Renderiza o conte√∫do da Etapa 5"""
    
    st.title("üìö ETAPA 5 ‚Äî RAG (Retrieval Augmented Generation)")
    st.markdown("**Dura√ß√£o:** Flex√≠vel")
    
    st.markdown("""
    RAG √© a t√©cnica mais importante para criar sistemas de IA empresariais com conhecimento espec√≠fico.
    """)
    
    # T√≥picos
    st.header("üìö O que voc√™ vai aprender:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        - üìä **Vetoriza√ß√£o de Dados**
        - üóÑÔ∏è **Chroma, Pinecone, Milvus**
        - üîÑ **Pipelines RAG**
        - üîç **Query Transformation**
        """)
    
    with col2:
        st.markdown("""
        - üìà **Re-ranking**
        - üéØ **Otimiza√ß√£o de Contexto**
        - üíæ **Chunking Strategies**
        - ‚ö° **Hybrid Search**
        """)
    
    st.success("üéØ **Resultado:** Capaz de criar sistemas empresariais com mem√≥ria e conhecimento espec√≠fico.")
    
    st.markdown("---")
    
    tabs = st.tabs([
        "Conceitos RAG",
        "Vector Databases",
        "Chunking",
        "Advanced RAG",
        "Exerc√≠cios"
    ])
    
    with tabs[0]:
        render_conceitos_rag()
    
    with tabs[1]:
        render_vector_dbs()
    
    with tabs[2]:
        render_chunking()
    
    with tabs[3]:
        render_advanced_rag()
    
    with tabs[4]:
        render_exercicios_etapa5()


def render_conceitos_rag():
    """Conceitos b√°sicos de RAG"""
    st.subheader("üìö O que √© RAG?")
    
    st.markdown("""
    ### Retrieval Augmented Generation
    
    RAG combina busca de informa√ß√µes com gera√ß√£o de texto para criar respostas baseadas em conhecimento espec√≠fico.
    
    **Fluxo b√°sico:**
    1. üìÑ Usu√°rio faz uma pergunta
    2. üîç Sistema busca documentos relevantes
    3. üìù Contexto √© adicionado ao prompt
    4. ü§ñ LLM gera resposta baseada no contexto
    5. ‚úÖ Resposta √© retornada com fontes
    """)
    
    code = """
from openai import OpenAI
from chromadb import Client
import chromadb

# Inicializar
client_openai = OpenAI()
chroma_client = chromadb.Client()

# Criar cole√ß√£o
collection = chroma_client.create_collection(name="documentos")

# 1. Adicionar documentos
documentos = [
    "Python √© uma linguagem de programa√ß√£o de alto n√≠vel.",
    "Machine Learning √© um subcampo da Intelig√™ncia Artificial.",
    "RAG combina busca e gera√ß√£o de texto.",
]

collection.add(
    documents=documentos,
    ids=[f"doc{i}" for i in range(len(documentos))]
)

# 2. Fazer pergunta
pergunta = "O que √© RAG?"

# 3. Buscar documentos relevantes
resultados = collection.query(
    query_texts=[pergunta],
    n_results=2
)

documentos_relevantes = resultados['documents'][0]

# 4. Criar prompt com contexto
contexto = "\\n".join(documentos_relevantes)
prompt = f\"\"\"
Contexto:
{contexto}

Pergunta: {pergunta}

Responda baseado no contexto fornecido:
\"\"\"

# 5. Gerar resposta
response = client_openai.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
"""
    
    st.code(code, language="python")
    
    st.info("""
    üí° **Vantagens do RAG:**
    - ‚úÖ Conhecimento atualizado
    - ‚úÖ Reduz alucina√ß√µes
    - ‚úÖ Cita fontes
    - ‚úÖ Dom√≠nio espec√≠fico
    """)


def render_vector_dbs():
    """Bancos de dados vetoriais"""
    st.subheader("üóÑÔ∏è Vector Databases")
    
    st.markdown("### Chroma - Simples e Local")
    
    code_chroma = """
import chromadb
from chromadb.utils import embedding_functions

# Cliente persistente
client = chromadb.PersistentClient(path="./chroma_db")

# Fun√ß√£o de embedding
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

# Criar cole√ß√£o
collection = client.create_collection(
    name="meus_docs",
    embedding_function=openai_ef,
    metadata={"description": "Documenta√ß√£o t√©cnica"}
)

# Adicionar documentos
collection.add(
    documents=[
        "FastAPI √© um framework web moderno para Python.",
        "Streamlit permite criar apps de dados rapidamente.",
        "Docker containeriza aplica√ß√µes para deploy."
    ],
    metadatas=[
        {"tipo": "framework", "linguagem": "python"},
        {"tipo": "ui", "linguagem": "python"},
        {"tipo": "devops", "linguagem": "agnostic"}
    ],
    ids=["doc1", "doc2", "doc3"]
)

# Buscar
resultados = collection.query(
    query_texts=["Como criar interfaces web?"],
    n_results=2,
    where={"linguagem": "python"}  # filtro
)

print(resultados['documents'])
print(resultados['distances'])
"""
    
    st.code(code_chroma, language="python")
    
    st.markdown("---")
    
    st.markdown("### Pinecone - Produ√ß√£o e Escala")
    
    code_pinecone = """
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI

# Inicializar
pc = Pinecone(api_key="your-pinecone-key")
openai_client = OpenAI()

# Criar √≠ndice
index_name = "meu-rag-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # dimens√£o do embedding
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

index = pc.Index(index_name)

# Fun√ß√£o para embedar
def get_embedding(text):
    response = openai_client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

# Upsert documentos
documentos = [
    {"id": "doc1", "text": "Python √© √≥timo para IA"},
    {"id": "doc2", "text": "JavaScript domina a web"},
]

vectors = []
for doc in documentos:
    vector = get_embedding(doc["text"])
    vectors.append({
        "id": doc["id"],
        "values": vector,
        "metadata": {"text": doc["text"]}
    })

index.upsert(vectors=vectors)

# Query
query = "linguagens para intelig√™ncia artificial"
query_vector = get_embedding(query)

results = index.query(
    vector=query_vector,
    top_k=3,
    include_metadata=True
)

for match in results['matches']:
    print(f"Score: {match['score']:.3f}")
    print(f"Text: {match['metadata']['text']}")
"""
    
    st.code(code_pinecone, language="python")
    
    st.markdown("### Compara√ß√£o:")
    
    import pandas as pd
    comparison = {
        "Database": ["Chroma", "Pinecone", "Milvus", "Weaviate", "Qdrant"],
        "Tipo": ["Local/Cloud", "Cloud", "Self-hosted", "Cloud/Self", "Cloud/Self"],
        "Uso": ["Dev/Pequeno", "Produ√ß√£o", "Grande escala", "Sem√¢ntico", "Performance"],
        "Facilidade": ["‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê"]
    }
    
    df = pd.DataFrame(comparison)
    st.table(df)


def render_chunking():
    """Estrat√©gias de chunking"""
    st.subheader("üíæ Chunking Strategies")
    
    st.markdown("""
    ### Por que Chunking √© Importante?
    
    Dividir documentos em peda√ßos menores melhora a relev√¢ncia da busca.
    """)
    
    code = """
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader

# 1. Character Text Splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\\n\\n", "\\n", " ", ""]
)

texto = \"\"\"
[seu texto longo aqui]
\"\"\"

chunks = splitter.split_text(texto)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i}: {len(chunk)} caracteres")

# 2. Token-based Splitter
from langchain.text_splitter import TokenTextSplitter

token_splitter = TokenTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

chunks_tokens = token_splitter.split_text(texto)

# 3. Semantic Chunking (por similaridade)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

semantic_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile"
)

semantic_chunks = semantic_splitter.split_text(texto)

# 4. Markdown/Code-aware Splitter
from langchain.text_splitter import MarkdownTextSplitter

md_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# Mant√©m estrutura do markdown
md_chunks = md_splitter.split_text(markdown_text)
"""
    
    st.code(code, language="python")
    
    st.markdown("### Melhores Pr√°ticas:")
    
    st.markdown("""
    - üìè **Tamanho:** 500-1000 tokens geralmente funciona bem
    - üîÑ **Overlap:** 10-20% para manter contexto
    - üìù **Metadados:** Adicione source, page, section
    - üéØ **Sem√¢ntico:** Considere quebrar por t√≥picos
    - üîç **Teste:** Diferentes estrat√©gias para seu dom√≠nio
    """)


def render_advanced_rag():
    """RAG avan√ßado"""
    st.subheader("üöÄ Advanced RAG Techniques")
    
    st.markdown("### 1. Query Transformation")
    
    code_query = """
from openai import OpenAI

client = OpenAI()

def expand_query(query):
    \"\"\"Expande query para melhorar busca\"\"\"
    prompt = f\"\"\"
    Dada a pergunta do usu√°rio, gere 3 vers√µes alternativas 
    que ajudariam a encontrar informa√ß√µes relevantes:
    
    Pergunta original: {query}
    
    Vers√µes alternativas (uma por linha):
    \"\"\"
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    
    alternativas = response.choices[0].message.content.strip().split('\\n')
    return [query] + alternativas

# Uso
query_original = "Como treinar modelos de ML?"
queries = expand_query(query_original)

# Buscar com todas as queries
all_results = []
for q in queries:
    results = collection.query(query_texts=[q], n_results=3)
    all_results.extend(results['documents'][0])

# Remover duplicatas e pegar top results
unique_results = list(set(all_results))[:5]
"""
    
    st.code(code_query, language="python")
    
    st.markdown("---")
    
    st.markdown("### 2. Re-ranking")
    
    code_rerank = """
from sentence_transformers import CrossEncoder

# Modelo de re-ranking
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_results(query, documents, top_k=3):
    \"\"\"Re-rankeia documentos por relev√¢ncia\"\"\"
    
    # Criar pares (query, doc)
    pairs = [[query, doc] for doc in documents]
    
    # Calcular scores
    scores = reranker.predict(pairs)
    
    # Ordenar por score
    ranked = sorted(zip(documents, scores), 
                   key=lambda x: x[1], 
                   reverse=True)
    
    return [doc for doc, score in ranked[:top_k]]

# Uso
query = "Como funciona RAG?"
initial_results = collection.query(query_texts=[query], n_results=10)
docs = initial_results['documents'][0]

# Re-rankear
final_docs = rerank_results(query, docs, top_k=3)
"""
    
    st.code(code_rerank, language="python")
    
    st.markdown("---")
    
    st.markdown("### 3. Hybrid Search (Keyword + Semantic)")
    
    code_hybrid = """
from rank_bm25 import BM25Okapi
import numpy as np

class HybridSearch:
    def __init__(self, documents, embeddings):
        self.documents = documents
        self.embeddings = np.array(embeddings)
        
        # Preparar BM25
        tokenized = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)
    
    def search(self, query, query_embedding, top_k=5, alpha=0.5):
        \"\"\"
        alpha: peso entre keyword (0) e semantic (1)
        \"\"\"
        
        # BM25 scores
        bm25_scores = self.bm25.get_scores(query.split())
        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())
        
        # Semantic scores
        query_emb = np.array(query_embedding)
        semantic_scores = np.dot(self.embeddings, query_emb)
        semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())
        
        # Combinar
        hybrid_scores = alpha * semantic_scores + (1 - alpha) * bm25_scores
        
        # Top K
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
        
        return [(self.documents[i], hybrid_scores[i]) for i in top_indices]

# Uso
searcher = HybridSearch(documents, embeddings)
results = searcher.search(query, query_embedding, alpha=0.7)
"""
    
    st.code(code_hybrid, language="python")
    
    st.success("""
    ‚úÖ **T√©cnicas Avan√ßadas:**
    - Multi-query retrieval
    - Parent document retrieval
    - Self-query
    - Contextual compression
    - Ensemble retrieval
    """)


def render_exercicios_etapa5():
    """Exerc√≠cios da Etapa 5"""
    st.subheader("üí™ Exerc√≠cios Pr√°ticos")
    
    exercicios = [
        {
            "titulo": "1. Sistema Q&A de Documenta√ß√£o",
            "descricao": "Crie um sistema RAG para responder perguntas sobre documenta√ß√£o t√©cnica.",
            "requisitos": [
                "Carregar m√∫ltiplos PDFs/docs",
                "Chunking inteligente",
                "Chroma ou Pinecone",
                "Busca sem√¢ntica",
                "Citar fontes com p√°ginas",
                "Interface Streamlit"
            ]
        },
        {
            "titulo": "2. RAG com Re-ranking",
            "descricao": "Implemente sistema RAG com re-ranking de resultados.",
            "requisitos": [
                "Initial retrieval com embedding",
                "Re-rank com CrossEncoder",
                "Comparar com/sem rerank",
                "M√©tricas de relev√¢ncia",
                "Query expansion",
                "Logging de queries"
            ]
        },
        {
            "titulo": "3. Hybrid Search System",
            "descricao": "Combine busca keyword e sem√¢ntica.",
            "requisitos": [
                "BM25 + embeddings",
                "Tuning do alpha",
                "Comparar abordagens",
                "Dataset de avalia√ß√£o",
                "M√©tricas: MRR, NDCG",
                "API de busca"
            ]
        },
        {
            "titulo": "4. RAG Empresarial",
            "descricao": "Sistema RAG completo para empresa.",
            "requisitos": [
                "M√∫ltiplas fontes de dados",
                "Filtros de metadados",
                "Permiss√µes de acesso",
                "Auditoria de queries",
                "Feedback loop",
                "Deploy em produ√ß√£o"
            ]
        }
    ]
    
    for ex in exercicios:
        with st.expander(f"{ex['titulo']}"):
            st.markdown(f"**Descri√ß√£o:** {ex['descricao']}")
            st.markdown("**Requisitos:**")
            for req in ex['requisitos']:
                st.markdown(f"- {req}")
    
    st.markdown("---")
    st.markdown("### üìã Checklist de Dom√≠nio:")
    
    checklist = [
        "Entendo o conceito e arquitetura RAG",
        "Sei usar vector databases (Chroma/Pinecone)",
        "Domino estrat√©gias de chunking",
        "Consigo implementar busca sem√¢ntica",
        "Sei fazer query transformation",
        "Entendo re-ranking",
        "Implementei hybrid search",
        "Sei avaliar qualidade do RAG"
    ]
    
    for i, item in enumerate(checklist):
        st.checkbox(item, key=f"check_etapa5_{i}")

