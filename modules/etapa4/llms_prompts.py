"""
ETAPA 4 ‚Äî LLMs + Engenharia de Prompt
M√≥dulo para ensinar Large Language Models e t√©cnicas de prompting
"""
import streamlit as st


def render_etapa4():
    """Renderiza o conte√∫do da Etapa 4"""
    
    st.title("ü§ñ ETAPA 4 ‚Äî LLMs + Engenharia de Prompt")
    st.markdown("**Dura√ß√£o:** 10 dias")
    
    st.markdown("""
    Domine os Large Language Models e aprenda a extrair o m√°ximo deles atrav√©s 
    de engenharia de prompt avan√ßada.
    """)
    
    # T√≥picos
    st.header("üìö O que voc√™ vai dominar:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        - üåê **OpenAI, Gemini, Claude, Llama**
        - üíª **Modelos Locais com Ollama**
        - üî§ **Tokeniza√ß√£o**
        - üìä **Embeddings**
        """)
    
    with col2:
        st.markdown("""
        - üí° **T√©cnicas de Prompting**
        - üéØ **Zero-shot, Few-shot, CoT**
        - üìù **Instru√ß√µes Personalizadas**
        - ü§ñ **Agentes de IA**
        """)
    
    st.success("üéØ **Resultado:** Capaz de construir chatbots e aplica√ß√µes avan√ßadas com LLMs.")
    
    st.markdown("---")
    
    tabs = st.tabs([
        "APIs de LLMs",
        "Ollama Local",
        "Tokeniza√ß√£o",
        "Prompting Avan√ßado",
        "Agentes",
        "Exerc√≠cios"
    ])
    
    with tabs[0]:
        render_apis_llms()
    
    with tabs[1]:
        render_ollama()
    
    with tabs[2]:
        render_tokenizacao()
    
    with tabs[3]:
        render_prompting()
    
    with tabs[4]:
        render_agentes()
    
    with tabs[5]:
        render_exercicios_etapa4()


def render_apis_llms():
    """Se√ß√£o de APIs de LLMs"""
    st.subheader("üåê APIs de LLMs Principais")
    
    st.markdown("### OpenAI API")
    
    code_openai = """
from openai import OpenAI
import os

# Inicializar cliente
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Chat completion
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "Voc√™ √© um assistente especializado em IA."},
        {"role": "user", "content": "Explique o que s√£o transformers em 3 linhas."}
    ],
    temperature=0.7,
    max_tokens=150
)

print(response.choices[0].message.content)

# Streaming
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Conte uma hist√≥ria curta"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")

# Embeddings
embedding_response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Texto para embedar"
)

embedding = embedding_response.data[0].embedding
print(f"\\nDimens√£o do embedding: {len(embedding)}")
"""
    
    st.code(code_openai, language="python")
    
    st.markdown("---")
    
    st.markdown("### Google Gemini API")
    
    code_gemini = """
import google.generativeai as genai
import os

# Configurar
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# Criar modelo
model = genai.GenerativeModel('gemini-pro')

# Gerar texto
response = model.generate_content("Explique computa√ß√£o qu√¢ntica")
print(response.text)

# Chat
chat = model.start_chat(history=[])

response = chat.send_message("Ol√°! Como voc√™ funciona?")
print(response.text)

response = chat.send_message("Me d√™ um exemplo de uso")
print(response.text)

# Ver hist√≥rico
for message in chat.history:
    print(f"{message.role}: {message.parts[0].text}")
"""
    
    st.code(code_gemini, language="python")
    
    st.markdown("---")
    
    st.markdown("### Anthropic Claude API")
    
    code_claude = """
import anthropic
import os

# Inicializar
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Chat
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Explique redes neurais de forma simples"}
    ]
)

print(message.content[0].text)

# Com system prompt
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system="Voc√™ √© um professor de IA que explica conceitos de forma clara.",
    messages=[
        {"role": "user", "content": "O que √© overfitting?"}
    ]
)

print(message.content[0].text)

# Streaming
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Conte uma piada sobre IA"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
"""
    
    st.code(code_claude, language="python")
    
    st.markdown("### Compara√ß√£o de Modelos:")
    
    comparison = {
        "Modelo": ["GPT-4o", "GPT-4o-mini", "Claude 3.5 Sonnet", "Gemini 1.5 Pro", "Llama 3.1"],
        "Contexto": ["128K tokens", "128K tokens", "200K tokens", "2M tokens", "128K tokens"],
        "Uso": ["Geral, complexo", "R√°pido, barato", "Reasoning, code", "Contexto longo", "Open source"]
    }
    
    import pandas as pd
    df = pd.DataFrame(comparison)
    st.table(df)


def render_ollama():
    """Se√ß√£o de Ollama"""
    st.subheader("üíª Ollama - LLMs Locais")
    
    st.markdown("""
    ### Rodar LLMs Localmente
    
    Ollama permite rodar modelos como Llama, Mistral, etc localmente.
    """)
    
    installation = """
# Instala√ß√£o
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows: baixar de https://ollama.com/download

# Verificar instala√ß√£o
ollama --version

# Baixar modelos
ollama pull llama3.1
ollama pull mistral
ollama pull codellama

# Listar modelos
ollama list

# Rodar modelo
ollama run llama3.1
"""
    
    st.code(installation, language="bash")
    
    st.markdown("### Usar Ollama com Python:")
    
    code = """
import requests
import json

# API endpoint
url = "http://localhost:11434/api/generate"

# Fazer requisi√ß√£o
data = {
    "model": "llama3.1",
    "prompt": "Explique machine learning em 3 linhas",
    "stream": False
}

response = requests.post(url, json=data)
result = response.json()
print(result['response'])

# Streaming
data["stream"] = True
response = requests.post(url, json=data, stream=True)

for line in response.iter_lines():
    if line:
        chunk = json.loads(line)
        print(chunk.get('response', ''), end='', flush=True)

# Usando biblioteca ollama
import ollama

# Gerar texto
response = ollama.generate(
    model='llama3.1',
    prompt='Por que o c√©u √© azul?'
)
print(response['response'])

# Chat
messages = [
    {'role': 'user', 'content': 'Por que Python √© popular em IA?'}
]

response = ollama.chat(model='llama3.1', messages=messages)
print(response['message']['content'])
"""
    
    st.code(code, language="python")
    
    st.success("""
    ‚úÖ **Vantagens do Ollama:**
    - 100% privado e offline
    - Sem custos de API
    - Controle total
    - R√°pido para testes
    """)


def render_tokenizacao():
    """Se√ß√£o de Tokeniza√ß√£o"""
    st.subheader("üî§ Tokeniza√ß√£o e Embeddings")
    
    st.markdown("""
    ### O que √© Tokeniza√ß√£o?
    
    LLMs n√£o entendem texto diretamente - eles processam tokens.
    """)
    
    code = """
import tiktoken

# Encoder para GPT-4
encoding = tiktoken.encoding_for_model("gpt-4")

# Tokenizar texto
text = "Intelig√™ncia Artificial est√° revolucionando o mundo!"
tokens = encoding.encode(text)

print(f"Texto: {text}")
print(f"Tokens: {tokens}")
print(f"N√∫mero de tokens: {len(tokens)}")

# Decodificar
decoded = encoding.decode(tokens)
print(f"Decodificado: {decoded}")

# Ver cada token
for token in tokens:
    print(f"{token} -> {encoding.decode([token])!r}")

# Calcular custo
def calcular_custo(num_tokens, preco_por_1k=0.03):
    return (num_tokens / 1000) * preco_por_1k

texto_longo = "..." * 1000  # seu texto
num_tokens = len(encoding.encode(texto_longo))
custo = calcular_custo(num_tokens)
print(f"\\nCusto estimado: ${custo:.4f}")
"""
    
    st.code(code, language="python")
    
    st.markdown("---")
    
    st.markdown("""
    ### Embeddings - Representa√ß√µes Vetoriais
    
    Embeddings capturam significado sem√¢ntico de textos.
    """)
    
    code_embeddings = """
from openai import OpenAI
import numpy as np

client = OpenAI()

def get_embedding(text, model="text-embedding-3-small"):
    response = client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

# Criar embeddings
textos = [
    "Cachorro √© um animal de estima√ß√£o",
    "Gato √© um pet dom√©stico",
    "Python √© uma linguagem de programa√ß√£o"
]

embeddings = [get_embedding(t) for t in textos]

# Calcular similaridade coseno
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# Comparar similaridades
for i, texto1 in enumerate(textos):
    for j, texto2 in enumerate(textos):
        if i < j:
            sim = cosine_similarity(embeddings[i], embeddings[j])
            print(f"Similaridade entre '{texto1}' e '{texto2}': {sim:.3f}")

# Busca sem√¢ntica
query = "animais dom√©sticos"
query_embedding = get_embedding(query)

for texto, emb in zip(textos, embeddings):
    sim = cosine_similarity(query_embedding, emb)
    print(f"'{texto}': {sim:.3f}")
"""
    
    st.code(code_embeddings, language="python")
    
    st.info("üí° **Embeddings s√£o a base de:** RAG, busca sem√¢ntica, recomenda√ß√µes!")


def render_prompting():
    """Se√ß√£o de Prompting Avan√ßado"""
    st.subheader("üí° T√©cnicas de Prompting Avan√ßado")
    
    st.markdown("### 1. Zero-Shot Prompting")
    
    zero_shot = """
prompt = \"\"\"
Classifique o sentimento do seguinte texto como Positivo, Negativo ou Neutro:

Texto: "Este produto superou minhas expectativas! Qualidade excelente."

Sentimento:
\"\"\"
"""
    
    st.code(zero_shot, language="python")
    
    st.markdown("### 2. Few-Shot Prompting")
    
    few_shot = """
prompt = \"\"\"
Classifique o sentimento dos textos:

Texto: "Adorei este filme, muito emocionante!"
Sentimento: Positivo

Texto: "P√©ssimo atendimento, n√£o recomendo."
Sentimento: Negativo

Texto: "O produto √© ok, nada excepcional."
Sentimento: Neutro

Texto: "Melhor compra que j√° fiz, super recomendo!"
Sentimento:
\"\"\"
"""
    
    st.code(few_shot, language="python")
    
    st.markdown("### 3. Chain-of-Thought (CoT)")
    
    cot = """
prompt = \"\"\"
Resolva este problema passo a passo:

Problema: Jo√£o tinha 15 ma√ß√£s. Ele deu 3 para Maria e comprou 8 mais. 
Depois comeu 2. Quantas ma√ß√£s Jo√£o tem agora?

Vamos pensar passo a passo:
1. Jo√£o come√ßou com: 15 ma√ß√£s
2. Deu 3 para Maria: 15 - 3 = 12 ma√ß√£s
3. Comprou 8 mais: 12 + 8 = 20 ma√ß√£s
4. Comeu 2: 20 - 2 = 18 ma√ß√£s

Resposta: Jo√£o tem 18 ma√ß√£s.

---

Agora resolva:
Pedro tinha 50 reais. Gastou 12 em almo√ßo, ganhou 20 do pai, 
e gastou 15 em um livro. Quanto Pedro tem agora?

Vamos pensar passo a passo:
\"\"\"
"""
    
    st.code(cot, language="python")
    
    st.markdown("### 4. ReAct (Reasoning + Acting)")
    
    react = """
prompt = \"\"\"
Voc√™ √© um assistente que pensa e age passo a passo.

Tarefa: Encontre a capital da Fran√ßa e sua popula√ß√£o.

Pensamento: Preciso encontrar a capital da Fran√ßa.
A√ß√£o: Buscar[Capital da Fran√ßa]
Observa√ß√£o: A capital da Fran√ßa √© Paris.

Pensamento: Agora preciso encontrar a popula√ß√£o de Paris.
A√ß√£o: Buscar[Popula√ß√£o de Paris]
Observa√ß√£o: Paris tem aproximadamente 2.2 milh√µes de habitantes.

Pensamento: Tenho todas as informa√ß√µes necess√°rias.
Resposta: A capital da Fran√ßa √© Paris, com popula√ß√£o de cerca de 2.2 milh√µes.
\"\"\"
"""
    
    st.code(react, language="python")
    
    st.markdown("### 5. Self-Consistency")
    
    self_consistency = """
# Gerar m√∫ltiplas respostas e escolher a mais comum
prompts = [
    "Resolva: 25 * 4 + 12 / 3 = ?",
    "Calcule passo a passo: 25 * 4 + 12 / 3",
    "Qual o resultado de 25 * 4 + 12 / 3?"
]

respostas = []
for prompt in prompts:
    # Gerar 3 respostas para cada
    for _ in range(3):
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        respostas.append(response.choices[0].message.content)

# Encontrar resposta mais comum (maioria)
from collections import Counter
resposta_final = Counter(respostas).most_common(1)[0][0]
"""
    
    st.code(self_consistency, language="python")
    
    st.success("""
    ‚úÖ **T√©cnicas Avan√ßadas:**
    - Tree of Thoughts
    - Self-Refine
    - ReWOO
    - Skeleton-of-Thought
    """)


def render_agentes():
    """Se√ß√£o de Agentes"""
    st.subheader("ü§ñ Agentes de IA")
    
    st.markdown("""
    ### O que s√£o Agentes?
    
    Agentes podem usar ferramentas e tomar decis√µes para completar tarefas.
    """)
    
    code = """
from openai import OpenAI
import json

client = OpenAI()

# Definir ferramentas
tools = [
    {
        "type": "function",
        "function": {
            "name": "buscar_clima",
            "description": "Busca informa√ß√µes do clima atual de uma cidade",
            "parameters": {
                "type": "object",
                "properties": {
                    "cidade": {
                        "type": "string",
                        "description": "Nome da cidade"
                    },
                    "unidade": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"]
                    }
                },
                "required": ["cidade"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calcular",
            "description": "Calcula express√µes matem√°ticas",
            "parameters": {
                "type": "object",
                "properties": {
                    "expressao": {
                        "type": "string",
                        "description": "Express√£o matem√°tica"
                    }
                },
                "required": ["expressao"]
            }
        }
    }
]

# Implementar fun√ß√µes
def buscar_clima(cidade, unidade="celsius"):
    # Simula√ß√£o - em produ√ß√£o usar API real
    return {
        "cidade": cidade,
        "temperatura": 25,
        "condicao": "Ensolarado",
        "unidade": unidade
    }

def calcular(expressao):
    try:
        resultado = eval(expressao)
        return {"resultado": resultado}
    except:
        return {"erro": "Express√£o inv√°lida"}

# Mapa de fun√ß√µes
available_functions = {
    "buscar_clima": buscar_clima,
    "calcular": calcular
}

# Agente em a√ß√£o
messages = [
    {"role": "user", "content": "Qual o clima em S√£o Paulo e quanto √© 25 * 4?"}
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)

# Processar tool calls
response_message = response.choices[0].message
tool_calls = response_message.tool_calls

if tool_calls:
    messages.append(response_message)
    
    for tool_call in tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)
        
        # Executar fun√ß√£o
        function_response = available_functions[function_name](**function_args)
        
        # Adicionar resultado
        messages.append({
            "tool_call_id": tool_call.id,
            "role": "tool",
            "name": function_name,
            "content": json.dumps(function_response)
        })
    
    # Gerar resposta final
    final_response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    
    print(final_response.choices[0].message.content)
"""
    
    st.code(code, language="python")
    
    st.markdown("### Tipos de Agentes:")
    
    st.markdown("""
    - üîß **ReAct Agent**: Reasoning + Acting
    - üéØ **Plan-and-Execute**: Planeja antes de agir
    - üîÑ **Multi-Agent**: M√∫ltiplos agentes colaborando
    - üå≥ **Tree-of-Thoughts**: Explora m√∫ltiplos caminhos
    """)


def render_exercicios_etapa4():
    """Exerc√≠cios da Etapa 4"""
    st.subheader("üí™ Exerc√≠cios Pr√°ticos")
    
    exercicios = [
        {
            "titulo": "1. Chatbot Avan√ßado",
            "descricao": "Crie um chatbot com mem√≥ria e personalidade.",
            "requisitos": [
                "Usar OpenAI ou Claude",
                "Implementar mem√≥ria de conversa√ß√£o",
                "System prompt bem definido",
                "Streaming de respostas",
                "Interface Streamlit",
                "Salvar hist√≥rico"
            ]
        },
        {
            "titulo": "2. Sistema de Classifica√ß√£o",
            "descricao": "Classifique tickets de suporte automaticamente.",
            "requisitos": [
                "Few-shot prompting",
                "Classificar em categorias",
                "Extrair informa√ß√µes chave",
                "Sugerir prioridade",
                "API FastAPI",
                "Logging com MLflow"
            ]
        },
        {
            "titulo": "3. Agente com Ferramentas",
            "descricao": "Crie um agente que usa m√∫ltiplas ferramentas.",
            "requisitos": [
                "M√≠nimo 3 ferramentas",
                "Busca na web",
                "C√°lculos matem√°ticos",
                "Acesso a banco de dados",
                "ReAct pattern",
                "Tratamento de erros"
            ]
        },
        {
            "titulo": "4. Sistema RAG B√°sico",
            "descricao": "Crie um sistema Q&A sobre documentos.",
            "requisitos": [
                "Carregar PDFs/textos",
                "Gerar embeddings",
                "Busca sem√¢ntica",
                "Gerar resposta com contexto",
                "Interface web",
                "Citar fontes"
            ]
        }
    ]
    
    for ex in exercicios:
        with st.expander(f"{ex['titulo']}"):
            st.markdown(f"**Descri√ß√£o:** {ex['descricao']}")
            st.markdown("**Requisitos:**")
            for req in ex['requisitos']:
                st.markdown(f"- {req}")
    
    st.markdown("---")
    st.markdown("### üìã Checklist de Dom√≠nio:")
    
    checklist = [
        "Sei usar APIs de LLMs principais",
        "Consigo rodar modelos locais com Ollama",
        "Entendo tokeniza√ß√£o e seus impactos",
        "Domino t√©cnicas de prompting",
        "Sei criar few-shot prompts efetivos",
        "Entendo Chain-of-Thought",
        "Consigo criar agentes com ferramentas",
        "Sei quando usar cada modelo/t√©cnica"
    ]
    
    for i, item in enumerate(checklist):
        st.checkbox(item, key=f"check_etapa4_{i}")

